{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5833ff03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import pickle\n",
    "import kfp.dsl as dsl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a12b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.components import func_to_container_op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b32a07a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def load_and_clean_data():\n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/TripathiAshutosh/dataset/main/banking.csv\")\n",
    "    print(\"Null/missingalues available in the data: \\n\", data.isna().sum())\n",
    "    data = data.dropna()\n",
    "    print(\"The data after dropping the NA values are: \\n\", data.isna().sum())\n",
    "    print(\"--------data imported and cleaned----------\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51e37f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_and_clean_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff650497",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def preprocessing(data):\n",
    "    data['education'] = np.where(data['education'].isin(['basic.9y', 'basic.6y', 'basic.4y']), 'Basic', data['education'])\n",
    "    categorical_vars = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "    for var in categorical_vars:\n",
    "        cat_list = pd.get_dummies(data[var], prefix=var) # one hot encoding\n",
    "        data = data.join(cat_list)\n",
    "    categorical_vars = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'day_of_week', 'poutcome']\n",
    "    data_vars = data.columns.values.tolist()\n",
    "    keeping = [i for i in data_vars if i not in categorical_vars]\n",
    "    final_df = data[keeping]\n",
    "    final_df.columns = final_df.columns.str.replace(\".\", \"_\").str.replace(\" \", \"_\")\n",
    "    print(final_df.head())\n",
    "    print(\"Education column pre-processed, categorical variables one-hot encoded. Ready to input data to model\")\n",
    "    return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf61306",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def train_test_split(final_df):\n",
    "    X = final_df.loc[:, final_df.columns != 'y']\n",
    "    y = final_df.loc[:, final_df.columns == 'y']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=47)\n",
    "    print(\"\\n---- X_train ----\\n\", X_train.head())\n",
    "    print(\"\\n---- X_test ----\\n\", X_test.head())\n",
    "    print(\"\\n---- y_test ----\\n\", y_test.head())\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ca01303",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def training_basic_classifier(X_train, y_train):\n",
    "    model = RandomForestClassifier(n_estimators=150)\n",
    "    model.fit(X_train, y_train.values.ravel())\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"n_estimators\", 150)\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        with open('data/model.pkl', 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "    print(\"\\nRandomForest classifier is trained on banking data and saved to PV location /data/model.pkl ----\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "938684bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "@func_to_container_op\n",
    "def predict_on_test_data(model, X_test):\n",
    "    print(\"---- Inside predict_on_test_data component ----\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    np.save('data/y_pred.npy', y_pred)\n",
    "    print(\"\\n---- Predicted classes ----\\n\", y_pred)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69abaacd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fba09a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name='My ML Pipeline')\n",
    "def my_pipeline():\n",
    "    # Define the pipeline steps\n",
    "    load_and_clean_dataa = load_and_clean_data()\n",
    "    data_preprocessing = preprocessing(load_and_clean_dataa.outputs[data])\n",
    "    data_splitting = train_test_split(data_preprocessing.output)\n",
    "    model_training = training_basic_classifier(data_splitting.outputs['X_train'])\n",
    "    prediction = predict_on_test_data(model_training.output, data_splitting.outputs['X_test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39dde949",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "TaskSpec(component_ref=ComponentReference(name=None, digest=None, tag=None, url=None, spec=ComponentSpec(name='Load and clean data', description=None, metadata=None, inputs=None, outputs=None, implementation=ContainerImplementation(container=ContainerSpec(image='python:3.7', command=['sh', '-ec', 'program_path=$(mktemp)\\nprintf \"%s\" \"$0\" > \"$program_path\"\\npython3 -u \"$program_path\" \"$@\"\\n', 'def load_and_clean_data():\\n    data = pd.read_csv(\"https://raw.githubusercontent.com/TripathiAshutosh/dataset/main/banking.csv\")\\n    print(\"Null/missingalues available in the data: \\\\n\", data.isna().sum())\\n    data = data.dropna()\\n    print(\"The data after dropping the NA values are: \\\\n\", data.isna().sum())\\n    print(\"--------data imported and cleaned----------\")\\n    return data\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\\'Load and clean data\\', description=\\'\\')\\n_parsed_args = vars(_parser.parse_args())\\n\\n_outputs = load_and_clean_data(**_parsed_args)\\n'], args=[], env=None, file_outputs=None)), version='google.com/cloud/pipelines/component/v1')), arguments={}, is_enabled=None, execution_options=None, annotations=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mkfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfinal.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m kfp\u001b[38;5;241m.\u001b[39mClient()\u001b[38;5;241m.\u001b[39mcreate_run_from_pipeline_func(my_pipeline)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\kfp\\compiler\\compiler.py:1175\u001b[0m, in \u001b[0;36mCompiler.compile\u001b[1;34m(self, pipeline_func, package_path, type_check, pipeline_conf)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1174\u001b[0m     kfp\u001b[38;5;241m.\u001b[39mTYPE_CHECK \u001b[38;5;241m=\u001b[39m type_check\n\u001b[1;32m-> 1175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_write_workflow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_conf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_conf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpackage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpackage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1180\u001b[0m     kfp\u001b[38;5;241m.\u001b[39mTYPE_CHECK \u001b[38;5;241m=\u001b[39m type_check_old_value\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\kfp\\compiler\\compiler.py:1227\u001b[0m, in \u001b[0;36mCompiler._create_and_write_workflow\u001b[1;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf, package_path)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_and_write_workflow\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1219\u001b[0m                                pipeline_func: Callable,\n\u001b[0;32m   1220\u001b[0m                                pipeline_name: Text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1223\u001b[0m                                pipeline_conf: dsl\u001b[38;5;241m.\u001b[39mPipelineConf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1224\u001b[0m                                package_path: Text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compile the given pipeline function and dump it to specified file\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m    format.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1227\u001b[0m     workflow \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mpipeline_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mpipeline_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1230\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_write_workflow(workflow, package_path)\n\u001b[0;32m   1231\u001b[0m     _validate_workflow(workflow)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mlops\\lib\\site-packages\\kfp\\compiler\\compiler.py:1005\u001b[0m, in \u001b[0;36mCompiler._create_workflow\u001b[1;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         args_list\u001b[38;5;241m.\u001b[39mappend(param)\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dsl\u001b[38;5;241m.\u001b[39mPipeline(pipeline_name) \u001b[38;5;28;01mas\u001b[39;00m dsl_pipeline:\n\u001b[1;32m-> 1005\u001b[0m     pipeline_func(\u001b[38;5;241m*\u001b[39margs_list, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs_dict)\n\u001b[0;32m   1007\u001b[0m pipeline_conf \u001b[38;5;241m=\u001b[39m pipeline_conf \u001b[38;5;129;01mor\u001b[39;00m dsl_pipeline\u001b[38;5;241m.\u001b[39mconf  \u001b[38;5;66;03m# Configuration passed to the compiler is overriding. Unfortunately, it's not trivial to detect whether the dsl_pipeline.conf was ever modified.\u001b[39;00m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_exit_handler(dsl_pipeline)\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mmy_pipeline\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;129m@dsl\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMy ML Pipeline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmy_pipeline\u001b[39m():\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Define the pipeline steps\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     load_and_clean_dataa \u001b[38;5;241m=\u001b[39m load_and_clean_data()\n\u001b[1;32m----> 5\u001b[0m     data_preprocessing \u001b[38;5;241m=\u001b[39m preprocessing(\u001b[43mload_and_clean_dataa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      6\u001b[0m     data_splitting \u001b[38;5;241m=\u001b[39m train_test_split(data_preprocessing\u001b[38;5;241m.\u001b[39moutput)\n\u001b[0;32m      7\u001b[0m     model_training \u001b[38;5;241m=\u001b[39m training_basic_classifier(data_splitting\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX_train\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: TaskSpec(component_ref=ComponentReference(name=None, digest=None, tag=None, url=None, spec=ComponentSpec(name='Load and clean data', description=None, metadata=None, inputs=None, outputs=None, implementation=ContainerImplementation(container=ContainerSpec(image='python:3.7', command=['sh', '-ec', 'program_path=$(mktemp)\\nprintf \"%s\" \"$0\" > \"$program_path\"\\npython3 -u \"$program_path\" \"$@\"\\n', 'def load_and_clean_data():\\n    data = pd.read_csv(\"https://raw.githubusercontent.com/TripathiAshutosh/dataset/main/banking.csv\")\\n    print(\"Null/missingalues available in the data: \\\\n\", data.isna().sum())\\n    data = data.dropna()\\n    print(\"The data after dropping the NA values are: \\\\n\", data.isna().sum())\\n    print(\"--------data imported and cleaned----------\")\\n    return data\\n\\nimport argparse\\n_parser = argparse.ArgumentParser(prog=\\'Load and clean data\\', description=\\'\\')\\n_parsed_args = vars(_parser.parse_args())\\n\\n_outputs = load_and_clean_data(**_parsed_args)\\n'], args=[], env=None, file_outputs=None)), version='google.com/cloud/pipelines/component/v1')), arguments={}, is_enabled=None, execution_options=None, annotations=None)"
     ]
    }
   ],
   "source": [
    "kfp.compiler.Compiler().compile(my_pipeline,'final.yaml')\n",
    "kfp.Client().create_run_from_pipeline_func(my_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c2f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddade32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a22d146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f16a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148e763",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8650e00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
